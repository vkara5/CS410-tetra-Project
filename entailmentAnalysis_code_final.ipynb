{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGne00G5OQ/8HGWx3JhqCA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-zy3XL7CuWye","executionInfo":{"status":"ok","timestamp":1670101424425,"user_tz":360,"elapsed":140966,"user":{"displayName":"vaibhav karanam","userId":"17342897047850185496"}},"outputId":"87707434-5698-4bdc-8b21-fa784e4fc8aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting allennlp-models\n","  Downloading allennlp_models-2.10.1-py3-none-any.whl (464 kB)\n","\u001b[K     |████████████████████████████████| 464 kB 28.9 MB/s \n","\u001b[?25hCollecting conllu==4.4.2\n","  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n","Collecting py-rouge==1.1\n","  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 1.1 MB/s \n","\u001b[?25hCollecting allennlp<2.11,>=2.10.1\n","  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n","\u001b[K     |████████████████████████████████| 730 kB 74.9 MB/s \n","\u001b[?25hCollecting word2number>=1.1\n","  Downloading word2number-1.1.zip (9.7 kB)\n","Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 984 kB/s \n","\u001b[?25hRequirement already satisfied: torch<1.13.0,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from allennlp-models) (1.12.1+cu113)\n","Collecting datasets\n","  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 91.0 MB/s \n","\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.8/dist-packages (from allennlp-models) (3.7)\n","Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (1.7.3)\n","Collecting tensorboardX>=1.2\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 89.5 MB/s \n","\u001b[?25hCollecting lmdb>=1.2.1\n","  Downloading lmdb-1.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (305 kB)\n","\u001b[K     |████████████████████████████████| 305 kB 93.8 MB/s \n","\u001b[?25hRequirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (0.3.6)\n","Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (1.21.6)\n","Collecting fairscale==0.4.6\n","  Downloading fairscale-0.4.6.tar.gz (248 kB)\n","\u001b[K     |████████████████████████████████| 248 kB 95.0 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (0.7.0)\n","Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (4.64.1)\n","Collecting h5py>=3.6.0\n","  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n","\u001b[K     |████████████████████████████████| 4.5 MB 78.7 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (0.13.1+cu113)\n","Collecting termcolor==1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","Collecting traitlets>5.1.1\n","  Downloading traitlets-5.6.0-py3-none-any.whl (107 kB)\n","\u001b[K     |████████████████████████████████| 107 kB 85.0 MB/s \n","\u001b[?25hCollecting filelock<3.8,>=3.3\n","  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n","Collecting jsonnet>=0.10.0\n","  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n","\u001b[K     |████████████████████████████████| 593 kB 68.9 MB/s \n","\u001b[?25hCollecting pytest>=6.2.5\n","  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n","\u001b[K     |████████████████████████████████| 316 kB 72.4 MB/s \n","\u001b[?25hCollecting huggingface-hub>=0.0.16\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 74.5 MB/s \n","\u001b[?25hCollecting requests>=2.28\n","  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.9 MB/s \n","\u001b[?25hCollecting sentencepiece>=0.1.96\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 75.9 MB/s \n","\u001b[?25hCollecting wandb<0.13.0,>=0.10.0\n","  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 80.7 MB/s \n","\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3\n","  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n","Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (1.0.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 96.2 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (3.19.6)\n","Collecting transformers<4.21,>=4.1\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 71.7 MB/s \n","\u001b[?25hCollecting base58>=2.1.1\n","  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n","Collecting spacy<3.4,>=2.1.0\n","  Downloading spacy-3.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 7.9 MB/s \n","\u001b[?25hRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (9.0.0)\n","Collecting huggingface-hub>=0.0.16\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 95.9 MB/s \n","\u001b[?25hCollecting rich<13.0,>=12.1\n","  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n","\u001b[K     |████████████████████████████████| 237 kB 97.0 MB/s \n","\u001b[?25hCollecting boto3<2.0,>=1.0\n","  Downloading boto3-1.26.22-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 89.0 MB/s \n","\u001b[?25hRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.5.0)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 11.6 MB/s \n","\u001b[?25hCollecting botocore<1.30.0,>=1.29.22\n","  Downloading botocore-1.29.22-py3-none-any.whl (10.2 MB)\n","\u001b[K     |████████████████████████████████| 10.2 MB 81.3 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 97.2 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.22->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.8.2)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.8.2)\n","Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.4.0)\n","Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.14.1)\n","Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.3.2)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.57.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (5.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (4.9)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.1->allennlp-models) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.1->allennlp-models) (4.1.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.1->allennlp-models) (6.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp-models) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp-models) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp-models) (7.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.1->allennlp-models) (3.0.9)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (0.4.8)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.1)\n","Collecting exceptiongroup>=1.0.0rc8\n","  Downloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\n","Collecting iniconfig\n","  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n","Collecting pluggy<2.0,>=0.12\n","  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (22.1.0)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp-models) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp-models) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp-models) (2022.9.24)\n","Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.6.1)\n","Collecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[K     |████████████████████████████████| 51 kB 9.6 MB/s \n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.1->allennlp<2.11,>=2.10.1->allennlp-models) (3.1.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.8)\n","Collecting thinc<8.1.0,>=8.0.14\n","  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n","\u001b[K     |████████████████████████████████| 671 kB 93.4 MB/s \n","\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (0.7.9)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.11.3)\n","Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n","  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[K     |████████████████████████████████| 13.7 MB 95.6 MB/s \n","\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (3.0.8)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.4.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (0.10.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (3.0.10)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (3.3.0)\n","Collecting typer>=0.4.1\n","  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (1.0.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (0.9.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (1.0.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (57.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.7)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (5.2.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp<2.11,>=2.10.1->allennlp-models) (7.1.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 82.6 MB/s \n","\u001b[?25hCollecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 79.0 MB/s \n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp-models) (5.4.8)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n","\u001b[K     |████████████████████████████████| 168 kB 95.0 MB/s \n","\u001b[?25hCollecting setproctitle\n","  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.3)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.8 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp-models) (9.0.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp-models) (2022.11.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 93.8 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp-models) (1.3.5)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->allennlp-models) (3.8.3)\n","Collecting xxhash\n","  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 98.1 MB/s \n","\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp-models) (1.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp-models) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp-models) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp-models) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->allennlp-models) (4.0.2)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->allennlp-models) (0.2.5)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->allennlp-models) (2022.6)\n","Building wheels for collected packages: fairscale, termcolor, jsonnet, word2number, pathtools, sacremoses\n","  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307251 sha256=d8217dea888499b0689f6d6c25931da94b16424d34a5e7a369d7cf35795cdf78\n","  Stored in directory: /root/.cache/pip/wheels/77/4c/a4/f6c0eec2ec5c8ffca075e62b0329801f862e1f1b71422f456b\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=8950cb9f3158eff35098e8fde0bf1d1257b026a16d08c03d75588e6c48beb996\n","  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.19.1-cp38-cp38-linux_x86_64.whl size=3996134 sha256=c5faef09915741e39b57937c92d0f510dca46f674185a2c496ab43682b448d43\n","  Stored in directory: /root/.cache/pip/wheels/64/ec/56/de861aae102c449ade2378772abbf9eb7e9acfe9a80f3e6036\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=ec4892a9d53d392a121cb38d603630363d0f62b7d1cbc9438fb26fbab67e45f0\n","  Stored in directory: /root/.cache/pip/wheels/cb/f3/5a/d88198fdeb46781ddd7e7f2653061af83e7adb2a076d8886d6\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=0d384f50a8f5408f714bc4912d7f9ff01ea0acf601877a13f2d24ed79421c90c\n","  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=e8a304f20a41cb926f530e36d752f03dd4eb143cd83c10b80dc61a0e5923d650\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built fairscale termcolor jsonnet word2number pathtools sacremoses\n","Installing collected packages: urllib3, requests, jmespath, smmap, botocore, typer, s3transfer, pydantic, gitdb, filelock, commonmark, tokenizers, thinc, shortuuid, setproctitle, sentry-sdk, rich, pluggy, pathtools, iniconfig, huggingface-hub, GitPython, exceptiongroup, docker-pycreds, boto3, xxhash, wandb, transformers, traitlets, termcolor, tensorboardX, spacy, sentencepiece, sacremoses, responses, pytest, multiprocess, lmdb, jsonnet, h5py, fairscale, cached-path, base58, word2number, py-rouge, ftfy, datasets, conllu, allennlp, allennlp-models\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.7.0\n","    Uninstalling typer-0.7.0:\n","      Successfully uninstalled typer-0.7.0\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.10.2\n","    Uninstalling pydantic-1.10.2:\n","      Successfully uninstalled pydantic-1.10.2\n","  Attempting uninstall: filelock\n","    Found existing installation: filelock 3.8.0\n","    Uninstalling filelock-3.8.0:\n","      Successfully uninstalled filelock-3.8.0\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.1.5\n","    Uninstalling thinc-8.1.5:\n","      Successfully uninstalled thinc-8.1.5\n","  Attempting uninstall: pluggy\n","    Found existing installation: pluggy 0.7.1\n","    Uninstalling pluggy-0.7.1:\n","      Successfully uninstalled pluggy-0.7.1\n","  Attempting uninstall: traitlets\n","    Found existing installation: traitlets 5.1.1\n","    Uninstalling traitlets-5.1.1:\n","      Successfully uninstalled traitlets-5.1.1\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 2.1.1\n","    Uninstalling termcolor-2.1.1:\n","      Successfully uninstalled termcolor-2.1.1\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.4.3\n","    Uninstalling spacy-3.4.3:\n","      Successfully uninstalled spacy-3.4.3\n","  Attempting uninstall: pytest\n","    Found existing installation: pytest 3.6.4\n","    Uninstalling pytest-3.6.4:\n","      Successfully uninstalled pytest-3.6.4\n","  Attempting uninstall: lmdb\n","    Found existing installation: lmdb 0.99\n","    Uninstalling lmdb-0.99:\n","      Successfully uninstalled lmdb-0.99\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\n","en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.3.1 which is incompatible.\u001b[0m\n","Successfully installed GitPython-3.1.29 allennlp-2.10.1 allennlp-models-2.10.1 base58-2.1.1 boto3-1.26.22 botocore-1.29.22 cached-path-1.1.6 commonmark-0.9.1 conllu-4.4.2 datasets-2.7.1 docker-pycreds-0.4.0 exceptiongroup-1.0.4 fairscale-0.4.6 filelock-3.7.1 ftfy-6.1.1 gitdb-4.0.10 h5py-3.7.0 huggingface-hub-0.10.1 iniconfig-1.1.1 jmespath-1.0.1 jsonnet-0.19.1 lmdb-1.3.0 multiprocess-0.70.14 pathtools-0.1.2 pluggy-1.0.0 py-rouge-1.1 pydantic-1.8.2 pytest-7.2.0 requests-2.28.1 responses-0.18.0 rich-12.6.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 sentry-sdk-1.11.1 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 spacy-3.3.1 tensorboardX-2.5.1 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 traitlets-5.6.0 transformers-4.20.1 typer-0.4.2 urllib3-1.26.13 wandb-0.12.21 word2number-1.1 xxhash-3.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (1.26.13)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting awscli\n","  Downloading awscli-1.27.22-py3-none-any.whl (3.9 MB)\n","\u001b[K     |████████████████████████████████| 3.9 MB 28.2 MB/s \n","\u001b[?25hCollecting six\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting botocore==1.29.22\n","  Using cached botocore-1.29.22-py3-none-any.whl (10.2 MB)\n","Collecting rsa<4.8,>=3.1.2\n","  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n","Collecting colorama<0.4.5,>=0.2.5\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Collecting docutils<0.17,>=0.10\n","  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n","\u001b[K     |████████████████████████████████| 548 kB 83.9 MB/s \n","\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n","  Using cached s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","Collecting PyYAML<5.5,>=3.10\n","  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n","\u001b[K     |████████████████████████████████| 662 kB 91.1 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Using cached urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","Collecting python-dateutil<3.0.0,>=2.1\n","  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n","\u001b[K     |████████████████████████████████| 247 kB 90.3 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting pyasn1>=0.1.3\n","  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.8 MB/s \n","\u001b[?25hInstalling collected packages: six, urllib3, python-dateutil, jmespath, pyasn1, botocore, s3transfer, rsa, PyYAML, docutils, colorama, awscli\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\n","Successfully installed PyYAML-6.0 awscli-1.27.22 botocore-1.29.22 colorama-0.4.4 docutils-0.17.1 jmespath-1.0.1 pyasn1-0.4.8 python-dateutil-2.8.2 rsa-4.9 s3transfer-0.6.0 six-1.16.0 urllib3-1.26.13\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dateutil","six"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (4.6.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (2.28.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests) (2022.9.24)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests) (1.26.13)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests) (2.10)\n"]}],"source":["!pip install allennlp-models\n","!pip install urllib3\n","!pip install awscli --ignore-installed six\n","#BeautifulSoup and requests\n","!pip install beautifulsoup4\n","!pip install requests"]},{"cell_type":"markdown","source":["<!-- {cancer:\n","{\n","  food:fish \n","  result: positve}\n","  } -->"],"metadata":{"id":"SBJhqxJrzL77"}},{"cell_type":"code","source":["from allennlp_models.pretrained import load_predictor\n","import numpy as np\n","import json"],"metadata":{"id":"CkOrJIRjusin","executionInfo":{"status":"ok","timestamp":1670101448947,"user_tz":360,"elapsed":9241,"user":{"displayName":"vaibhav karanam","userId":"17342897047850185496"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JU-eBE1ugOF","executionInfo":{"status":"ok","timestamp":1670101474724,"user_tz":360,"elapsed":22387,"user":{"displayName":"vaibhav karanam","userId":"17342897047850185496"}},"outputId":"9a2321cc-c082-4c02-c417-3715fb5b3ea3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["## Function for deciding if a food is beneficial or detrimental\n","\n","# Uses the result of the prob_for_single_food/prob_for_single_food_single_line dict in order to decide\n","# If max(pos_entailment_prob) > max(pos_neutral_prob) and max(neg_entailment_prob) -> Sort food as \"positive\"\n","# If max(neg_entailment_prob) > max(neg_neutral_prob) and max(pos_entailment_prob) -> Sort food as \"negative\"\n","# Else -> Sort food as \"neutral\"\n","\n","# if at some point check each line\n","# // pos entailment > pos neutral and pos entailment>0.8 => pos\n","# neg entailment > neg neutral and neg entailment>0.8 => neg\n","# pos contradiction > pos neutral and pos contra>0.8 = > neg\n","# neg contradiction > neg neutral and neg contra>0.8 = > pos\n","\n","def sort_food(prob_dict, threshold = 0.8): # Takes in a dictionary of probabilities, along with a threshold value, returns a String result and the probability\n","  # pos_prob = max(prob_dict['pos']['entailment'])\n","  # pos_neutral_prob = max(prob_dict['pos']['neutral'])\n","  # neg_prob = max(prob_dict['neg']['entailment'])\n","  # neg_neutral_prob = max(prob_dict['neg']['neutral'])\n","  pos_entailment = prob_dict['pos']['entailment']\n","  pos_neutral = prob_dict['pos']['neutral']\n","  pos_contradiction = prob_dict['pos']['contradiction']\n","\n","  neg_entailment = prob_dict['neg']['entailment']\n","  neg_neutral = prob_dict['neg']['neutral']\n","  neg_contradiction = prob_dict['neg']['contradiction']\n","\n","  for i in range(len(pos_entailment)): # We need separate loops since the number of sentences used for positive sentiment may be different than negative\n","    if pos_entailment[i] > pos_neutral[i] and pos_entailment[i] >= threshold:\n","      return \"Positive\", pos_entailment[i]\n","    elif pos_contradiction[i] > pos_neutral[i] and pos_contradiction[i] >= threshold:\n","      return \"Negative\", pos_contradiction[i]\n","  \n","  for i in range(len(neg_entailment)):\n","    if neg_entailment[i] > neg_neutral[i] and neg_entailment[i] >= threshold:\n","      return \"Negative\", neg_entailment[i]\n","    elif neg_contradiction[i] > neg_neutral[i] and neg_contradiction[i] >= threshold:\n","      return \"Positive\", neg_contradiction[i]\n","\n","  return \"Neutral\", max(pos_neutral)\n","\n","  # if (pos_prob > pos_neutral_prob and pos_prob > neg_prob):\n","  #   return \"Positive\", pos_prob # Should something else be done here as a double check?\n","  # elif (neg_prob > neg_neutral_prob and neg_prob > pos_prob):\n","  #   return \"Negative\", neg_prob # Should something else be done here as a double check?\n","  # else:\n","  #   return \"Neutral\", pos_neutral_prob # If those conditions aren't met, then we can say that the outcome is inconclusive and therefore neutral\n"],"metadata":{"id":"2yuW0ca-ukID","executionInfo":{"status":"ok","timestamp":1670101477910,"user_tz":360,"elapsed":280,"user":{"displayName":"vaibhav karanam","userId":"17342897047850185496"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# food_name - str, name of a single food ('ground chicken')\n","# diet_reco_lines - iterable (list) [each item will be a str, a single sentence froma  Wiki paragraph]\n","\n","def prob_for_single_food(food_name, diet_reco_lines): # a single food-disease pair\n","    predictor = load_predictor(\"pair-classification-roberta-snli\")\n","    print('Diet recommendation has', len(diet_reco_lines), 'lines')\n","\n","    # Positive association check\n","    pos_entailment_prob = [] # entailment_prob for a pos association of food_name with each line in diet_reco_lines\n","    pos_contradiction_prob = [] # contradiction_prob for a pos association of food_name with each line in diet_reco_lines\n","    pos_neutral_prob = [] # neutral_prob for a pos association of food_name with each line in diet_reco_lines\n","    # pos_phrases = ['should be chosen']\n","    pos_phrases = ['is good', 'is recommended', 'is beneficial', 'is healthy', 'is useful', 'is highly recommended', 'is highly beneficial', 'is very healthy', 'is very good', 'is very useful','should be chosen']\n","    for line in diet_reco_lines:\n","        premise = line\n","        ep = [] # 10 entailment prob\n","        cp = []\n","        np = []\n","        for phrase in pos_phrases:\n","            hypothesis = food_name + ' ' + phrase\n","            probs = predictor.predict(premise, hypothesis)[\"probs\"]\n","            ep.append(probs[0])\n","            cp.append(probs[1])\n","            np.append(probs[2])\n","        pos_entailment_prob.append(max(ep))\n","        pos_contradiction_prob.append(max(cp))\n","        pos_neutral_prob.append(max(np))\n","\n","    # Negative association check\n","    neg_entailment_prob = [] # entailment_prob for a neg association of food_name with each line in diet_reco_lines\n","    neg_contradiction_prob = [] # contradiction_prob for a neg association of food_name with each line in diet_reco_lines\n","    neg_neutral_prob = [] # neutral_prob for a neg association of food_name with each line in diet_reco_lines\n","    # neg_phrases = ['should not be chosen']\n","    neg_phrases = ['is risky', 'is harmful', 'is very harmful', 'is discouraged', 'is highly discouraged', 'is very risky', 'is unhealthy', 'is very unhealthy', 'is bad', 'is very bad','should not be chosen']\n","    for line in diet_reco_lines:\n","        premise = line\n","        ep = []\n","        cp = []\n","        np = []\n","        for phrase in neg_phrases:\n","            hypothesis = food_name + ' ' + phrase\n","            probs = predictor.predict(premise, hypothesis)[\"probs\"]\n","            ep.append(probs[0])\n","            cp.append(probs[1])\n","            np.append(probs[2])\n","        neg_entailment_prob.append(max(ep))\n","        neg_contradiction_prob.append(max(cp))\n","        neg_neutral_prob.append(max(np))\n","\n","    prob_dict = {\n","        'pos': {\n","            'entailment': pos_entailment_prob,\n","            'contradiction': pos_contradiction_prob,\n","            'neutral': pos_neutral_prob\n","        },\n","        'neg': {\n","            'entailment': neg_entailment_prob,\n","            'contradiction': neg_contradiction_prob,\n","            'neutral': neg_neutral_prob\n","        }\n","    }\n","\n","    # prob_arr = np.array([\n","    #     [pos_entailment_prob, pos_contradiction_prob, pos_neutral_prob],\n","    #     [neg_entailment_prob, neg_contradiction_prob, neg_neutral_prob]\n","    # ])\n","\n","    #return prob_arr\n","    print(prob_dict)\n","    return prob_dict"],"metadata":{"id":"wvkUa0ZIu0ld","executionInfo":{"status":"ok","timestamp":1670101481601,"user_tz":360,"elapsed":4,"user":{"displayName":"vaibhav karanam","userId":"17342897047850185496"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["file = open(\"/content/drive/MyDrive/diseases_diet.json\", 'r')\n","\n","json_data = json.load(file)\n","\n","# print(json_data[\"Diseases\"])\n","\n","#lis = []\n","\n","# for disease in json_data:\n","#   # print(disease)\n","#   for line in json_data[disease]:\n","\n","#     print(i)\n","  \n","file.close()"],"metadata":{"id":"WR6Mn3n3u6kz","executionInfo":{"status":"ok","timestamp":1670101487478,"user_tz":360,"elapsed":1218,"user":{"displayName":"vaibhav karanam","userId":"17342897047850185496"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/food_subgroup.csv' ##Use the link to the file link in google drive//Drive mount in local drive\n","\n","import pandas as pd\n","import io\n","df = pd.read_csv(path)\n","#print(df)\n","\n","df2 = df[['food_subgroup']]\n","# print(df2)\n","\n","col_list = df2['food_subgroup'].values.tolist() #col_list can be iterated to get food item.\n","#print(col_list)"],"metadata":{"id":"M9-vFc0YvQSo","executionInfo":{"status":"ok","timestamp":1670101492611,"user_tz":360,"elapsed":1200,"user":{"displayName":"vaibhav karanam","userId":"17342897047850185496"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from tables.tests.common import ne\n","#List - all the lines:\n","sentiment_list =[]\n","neutral_list =[]\n","\n","for disease in json_data:\n","  # print(disease)\n","  for line in json_data[disease]:\n","    for food in col_list:\n","      if food.lower() in line.lower():\n","        sentiment_list.append(food)\n","      else:\n","         neutral_list.append(food)\n","\n","\n","neutral_list = [*set(neutral_list)]\n","sentiment_list = [*set(sentiment_list)]\n","\n","dict_lines = {}\n","\n","for food in sentiment_list:\n","  for disease in json_data:\n","    for line in json_data[disease]:\n","      if food in line:\n","        dict_lines[disease] = line\n","\n","print(dict_lines)\n","\n","\n","# print(str(len(neutral_list)))\n","for x in sentiment_list:\n","  print(str(x))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BROmqg8AvUgc","executionInfo":{"status":"ok","timestamp":1670101496822,"user_tz":360,"elapsed":292,"user":{"displayName":"vaibhav karanam","userId":"17342897047850185496"}},"outputId":"acd496bf-e43c-4b99-c193-50b253a86285"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Stroke': 'Nutrition, specifically the Mediterranean-style diet, has the potential for decreasing the risk of having a stroke by more than half. It does not appear that lowering levels of homocysteine with folic acid affects the risk of stroke.', 'Dementia': 'The Mediterranean and DASH diets are both associated with less cognitive decline. A different approach has been to incorporate elements of both of these diets into one known as the MIND diet. These diets are generally low in saturated fats while providing a good source of carbohydrates, mainly those that help stabilize blood sugar and insulin levels. Raised blood sugar levels over a long time, can damage nerves and cause memory problems if they are not managed. Nutritional factors associated with the proposed diets for reducing dementia risk include unsaturated fatty acids, vitamin E, vitamin C, flavonoids, vitamin B, and vitamin D.', 'Gallstone': 'Nutritional factors that may increase risk of gallstones include constipation; eating fewer meals per day; low intake of the nutrients folate, magnesium, calcium, and vitamin C; low fluid consumption; and, at least for men, a high intake of carbohydrate, a high glycemic load, and high glycemic index diet. Wine and whole-grained bread may decrease the risk of gallstones.', 'Prostate cancer': 'Fish may lower prostate-cancer deaths, but does not appear to affect occurrence. Some evidence supports lower rates of prostate cancer with a vegetarian diet, lycopene, selenium cruciferous vegetables, soy, beans and/or other legumes.', 'Cancer': 'While many dietary recommendations have been proposed to reduce cancer risks, the evidence to support them is not definitive. The primary dietary factors that increase risk are obesity and alcohol consumption. Diets low in fruits and vegetables and high in red meat have been implicated but reviews and meta-analyses do not come to a consistent conclusion. A 2014 meta-analysis found no relationship between fruits and vegetables and cancer. Coffee is associated with a reduced risk of liver cancer. Studies have linked excessive consumption of red or processed meat to an increased risk of breast cancer, colon cancer and pancreatic cancer, a phenomenon that could be due to the presence of carcinogens in meats cooked at high temperatures. In 2015 the IARC reported that eating processed meat  and, to a lesser degree, red meat was linked to some cancers.', 'Coronary artery disease': 'In 2016 research into the archives of theSugar Association, the trade association for the sugar industry in the US, had sponsored an influential literature review published in 1965 in the New England Journal of Medicine that downplayed early findings about the role of a diet heavy in sugar in the development of CAD and emphasized the role of fat; that review influenced decades of research funding and guidance on healthy eating.', 'Type 2 diabetes': 'Blood glucose levels can also be normalized in diabetic rodents by a single intrahypothalamic infusion of Fibroblast Growth Factor 1 , an effect that persists for months even in severely diabetic animals. This remarkable cure of diabetes is accomplished by a stimulation of accessory brain cells called astrocytes.  Hypothalamic astrocytes that produce Fatty Acid Binding Protein 7  are targets of FGF1; these cells are also in close contact with leptin-sensitive neurons, influence their function, and regulate leptin sensitivity. An abnormal function of FABP7+ astrocytes thus may contribute to the resistance to leptin and insulin that appear during aging and during exposure to high-fat diets.', \"Crohn's disease\": \"Certain lifestyle changes can reduce symptoms, including dietary adjustments, elemental diet, proper hydration, and smoking cessation. Patients with Crohn's disease are very interested in diet. Recent reviews underlined the importance to adopt diets that are best supported by evidence, even if little is known about the impact of diets on these patients.Diets that include higher levels of fiber and fruit are associated with reduced risk, while diets rich in total fats, polyunsaturated fatty acids, meat, and omega-6 fatty acids may increase the risk of Crohn's. Maintaining a balanced diet with proper portion control can help manage symptoms of the disease. Eating small meals frequently instead of big meals may also help with a low appetite. A food diary may help with identifying foods that trigger symptoms. Despite the recognized importance of dietary fiber for intestinal health, some people should follow a low residue diet to control acute symptoms especially if foods high in insoluble fiber cause symptoms, e.g., due to obstruction or irritation of the bowel. Some find relief in eliminating casein  and gluten  from their diets. They may have specific dietary intolerances , for example, lactose. Fatigue can be helped with regular exercise, a healthy diet, and enough sleep, and for those with malabsorption of vitamin B12 due to disease or surgical resection of the terminal ileum, cobalamin injections. Smoking may worsen symptoms and the course of the disease, and stopping is recommended. Alcohol consumption can also worsen symptoms, and moderation or cessation is advised.\"}\n","Vegetable\n","Mediterranean Diet\n","Fruit\n","Seaweed\n","Bean\n","Legume\n","Nut\n","Fish\n","Beans\n","Fruits\n","Soy\n","Oil\n","Coffee\n","Spread\n","Animal fats\n","Oils\n","Vegetables\n","Nuts\n","Meats\n","Alcoholic beverages\n","Sugar\n","Proteins\n","Protein\n","Roe\n","Ovis\n","Citrus\n","Mushroom\n","Tea\n","Mushrooms\n","Fats and oils\n","Fat\n","Beverages\n","Meat\n","Legumes\n","Animal fat\n","Fats\n","Sugars\n","Carb\n"]}]},{"cell_type":"code","source":["output_dict ={}\n","for disease in dict_lines:\n","  print(dict_lines[disease])\n","  output_dict[disease] = {}\n","  for food in sentiment_list:\n","      dict_tool = prob_for_single_food(food,[dict_lines[disease]])\n","      txt, prob_num =  sort_food(dict_tool)\n","      # output_dict[disease][\"food\"] = food\n","      output_dict[disease][food] = txt\n","      print(food+\" \"+\" Txt: \"+txt+\"Prob: \"+str(prob_num))\n","\n","second_dict = output_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"739S54KIvbWt","outputId":"795ffb2a-7c69-4495-d2ae-bcb5c887e273"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Nutrition, specifically the Mediterranean-style diet, has the potential for decreasing the risk of having a stroke by more than half. It does not appear that lowering levels of homocysteine with folic acid affects the risk of stroke.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0025365285109728575], 'contradiction': [0.9958754181861877], 'neutral': [0.019484316930174828]}, 'neg': {'entailment': [0.0013569140573963523], 'contradiction': [0.9969733953475952], 'neutral': [0.003533224808052182]}}\n","Vegetable  Txt: NegativeProb: 0.9958754181861877\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.08307702839374542], 'contradiction': [0.9903610944747925], 'neutral': [0.09252802282571793]}, 'neg': {'entailment': [0.0014678792795166373], 'contradiction': [0.9966975450515747], 'neutral': [0.003032419364899397]}}\n","Mediterranean Diet  Txt: NegativeProb: 0.9903610944747925\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0016466378001496196], 'contradiction': [0.9950633645057678], 'neutral': [0.020404180511832237]}, 'neg': {'entailment': [0.001560427132062614], 'contradiction': [0.9965772032737732], 'neutral': [0.004900775384157896]}}\n","Fruit  Txt: NegativeProb: 0.9950633645057678\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0012345786672085524], 'contradiction': [0.9970843195915222], 'neutral': [0.0060095167718827724]}, 'neg': {'entailment': [0.0013989864382892847], 'contradiction': [0.9972758889198303], 'neutral': [0.0026431235019117594]}}\n","Seaweed  Txt: NegativeProb: 0.9970843195915222\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0023639542050659657], 'contradiction': [0.9949786067008972], 'neutral': [0.010779724456369877]}, 'neg': {'entailment': [0.0013073207810521126], 'contradiction': [0.9965930581092834], 'neutral': [0.002741111209616065]}}\n","Bean  Txt: NegativeProb: 0.9949786067008972\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0022608956787735224], 'contradiction': [0.993907630443573], 'neutral': [0.01301068440079689]}, 'neg': {'entailment': [0.0014765039086341858], 'contradiction': [0.9966244697570801], 'neutral': [0.0031423396430909634]}}\n","Legume  Txt: NegativeProb: 0.993907630443573\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.04887814447283745], 'contradiction': [0.9929510354995728], 'neutral': [0.032207854092121124]}, 'neg': {'entailment': [0.00207286118529737], 'contradiction': [0.9968041181564331], 'neutral': [0.002947046421468258]}}\n","Nut  Txt: NegativeProb: 0.9929510354995728\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001957530388608575], 'contradiction': [0.996322751045227], 'neutral': [0.018470169976353645]}, 'neg': {'entailment': [0.0013545278925448656], 'contradiction': [0.9968519806861877], 'neutral': [0.0029487167485058308]}}\n","Fish  Txt: NegativeProb: 0.996322751045227\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0015063855098560452], 'contradiction': [0.996006429195404], 'neutral': [0.006208374630659819]}, 'neg': {'entailment': [0.001343773677945137], 'contradiction': [0.9967705011367798], 'neutral': [0.002513598185032606]}}\n","Beans  Txt: NegativeProb: 0.996006429195404\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0020903439726680517], 'contradiction': [0.9958005547523499], 'neutral': [0.014882155694067478]}, 'neg': {'entailment': [0.0015220604836940765], 'contradiction': [0.9967125654220581], 'neutral': [0.003416476072743535]}}\n","Fruits  Txt: NegativeProb: 0.9958005547523499\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0013179121306166053], 'contradiction': [0.9963119626045227], 'neutral': [0.0062715839594602585]}, 'neg': {'entailment': [0.0012200827477499843], 'contradiction': [0.9968445301055908], 'neutral': [0.0029065092094242573]}}\n","Soy  Txt: NegativeProb: 0.9963119626045227\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002101738005876541], 'contradiction': [0.9963852167129517], 'neutral': [0.006592817138880491]}, 'neg': {'entailment': [0.003454821417108178], 'contradiction': [0.9967939257621765], 'neutral': [0.007300178520381451]}}\n","Oil  Txt: NegativeProb: 0.9963852167129517\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0015926233027130365], 'contradiction': [0.9966697096824646], 'neutral': [0.009922102093696594]}, 'neg': {'entailment': [0.0022874982096254826], 'contradiction': [0.9972238540649414], 'neutral': [0.006616133265197277]}}\n","Coffee  Txt: NegativeProb: 0.9966697096824646\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.007483975030481815], 'contradiction': [0.9954444169998169], 'neutral': [0.011928382329642773]}, 'neg': {'entailment': [0.0018712590681388974], 'contradiction': [0.9965696334838867], 'neutral': [0.004593405872583389]}}\n","Spread  Txt: NegativeProb: 0.9954444169998169\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0028263283893465996], 'contradiction': [0.9956455826759338], 'neutral': [0.009833605028688908]}, 'neg': {'entailment': [0.0019526672549545765], 'contradiction': [0.9964824914932251], 'neutral': [0.0062014441937208176]}}\n","Animal fats  Txt: NegativeProb: 0.9956455826759338\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0019146095728501678], 'contradiction': [0.9962332844734192], 'neutral': [0.006224861368536949]}, 'neg': {'entailment': [0.0017898156074807048], 'contradiction': [0.9967833757400513], 'neutral': [0.0030651120468974113]}}\n","Oils  Txt: NegativeProb: 0.9962332844734192\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0021713608875870705], 'contradiction': [0.9954889416694641], 'neutral': [0.02300839126110077]}, 'neg': {'entailment': [0.0013519750209525228], 'contradiction': [0.9967561364173889], 'neutral': [0.0037930968683212996]}}\n","Vegetables  Txt: NegativeProb: 0.9954889416694641\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.005970513913780451], 'contradiction': [0.9958414435386658], 'neutral': [0.008645610883831978]}, 'neg': {'entailment': [0.0013999385992065072], 'contradiction': [0.9969967603683472], 'neutral': [0.0023506516590714455]}}\n","Nuts  Txt: NegativeProb: 0.9958414435386658\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001998767489567399], 'contradiction': [0.9964839220046997], 'neutral': [0.0040678842924535275]}, 'neg': {'entailment': [0.001317539601586759], 'contradiction': [0.9969782829284668], 'neutral': [0.00230435305275023]}}\n","Meats  Txt: NegativeProb: 0.9964839220046997\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001679693115875125], 'contradiction': [0.99659663438797], 'neutral': [0.037232086062431335]}, 'neg': {'entailment': [0.004449412226676941], 'contradiction': [0.9964678287506104], 'neutral': [0.01973371021449566]}}\n","Alcoholic beverages  Txt: NegativeProb: 0.99659663438797\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0020271893590688705], 'contradiction': [0.9960479140281677], 'neutral': [0.011208176612854004]}, 'neg': {'entailment': [0.0025645550340414047], 'contradiction': [0.9964057207107544], 'neutral': [0.012217888608574867]}}\n","Sugar  Txt: NegativeProb: 0.9960479140281677\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.006236849818378687], 'contradiction': [0.9954719543457031], 'neutral': [0.007021804340183735]}, 'neg': {'entailment': [0.0016273900400847197], 'contradiction': [0.9966983199119568], 'neutral': [0.0027987444773316383]}}\n","Proteins  Txt: NegativeProb: 0.9954719543457031\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0029631617944687605], 'contradiction': [0.994255006313324], 'neutral': [0.015453158877789974]}, 'neg': {'entailment': [0.0034212307073175907], 'contradiction': [0.9965315461158752], 'neutral': [0.00964303221553564]}}\n","Protein  Txt: NegativeProb: 0.994255006313324\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0015694097382947803], 'contradiction': [0.9962173104286194], 'neutral': [0.0074523864313960075]}, 'neg': {'entailment': [0.001251407666131854], 'contradiction': [0.9968844056129456], 'neutral': [0.003954458516091108]}}\n","Roe  Txt: NegativeProb: 0.9962173104286194\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0021121737081557512], 'contradiction': [0.9955339431762695], 'neutral': [0.010591438971459866]}, 'neg': {'entailment': [0.0015047601191326976], 'contradiction': [0.9967108964920044], 'neutral': [0.003919272217899561]}}\n","Ovis  Txt: NegativeProb: 0.9955339431762695\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001613097032532096], 'contradiction': [0.9961812496185303], 'neutral': [0.010683593340218067]}, 'neg': {'entailment': [0.0015093463007360697], 'contradiction': [0.9968427419662476], 'neutral': [0.0037288768216967583]}}\n","Citrus  Txt: NegativeProb: 0.9961812496185303\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0013494545128196478], 'contradiction': [0.9964427351951599], 'neutral': [0.010048705153167248]}, 'neg': {'entailment': [0.0013678872492164373], 'contradiction': [0.9968812465667725], 'neutral': [0.0037223019171506166]}}\n","Mushroom  Txt: NegativeProb: 0.9964427351951599\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001578312017954886], 'contradiction': [0.996191143989563], 'neutral': [0.007179890293627977]}, 'neg': {'entailment': [0.0014365310780704021], 'contradiction': [0.9969481825828552], 'neutral': [0.0028301209677010775]}}\n","Tea  Txt: NegativeProb: 0.996191143989563\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0012949659721925855], 'contradiction': [0.9959326386451721], 'neutral': [0.012477868236601353]}, 'neg': {'entailment': [0.0013887598179280758], 'contradiction': [0.9967710375785828], 'neutral': [0.0036435683723539114]}}\n","Mushrooms  Txt: NegativeProb: 0.9959326386451721\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.010020885616540909], 'contradiction': [0.9947966933250427], 'neutral': [0.013074762187898159]}, 'neg': {'entailment': [0.0016097507905215025], 'contradiction': [0.9969772100448608], 'neutral': [0.0027352168690413237]}}\n","Fats and oils  Txt: NegativeProb: 0.9947966933250427\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0023673761170357466], 'contradiction': [0.9963892698287964], 'neutral': [0.005697543732821941]}, 'neg': {'entailment': [0.001745961606502533], 'contradiction': [0.9965766072273254], 'neutral': [0.004954008385539055]}}\n","Fat  Txt: NegativeProb: 0.9963892698287964\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0071913935244083405], 'contradiction': [0.9940808415412903], 'neutral': [0.030361909419298172]}, 'neg': {'entailment': [0.0018588079838082194], 'contradiction': [0.9966347813606262], 'neutral': [0.004380641505122185]}}\n","Beverages  Txt: NegativeProb: 0.9940808415412903\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0016011998523026705], 'contradiction': [0.9961771965026855], 'neutral': [0.010270622558891773]}, 'neg': {'entailment': [0.0026940780226141214], 'contradiction': [0.9968851208686829], 'neutral': [0.008494113571941853]}}\n","Meat  Txt: NegativeProb: 0.9961771965026855\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0028910902328789234], 'contradiction': [0.9955717325210571], 'neutral': [0.007215813267976046]}, 'neg': {'entailment': [0.0014216969721019268], 'contradiction': [0.9967024922370911], 'neutral': [0.00305538740940392]}}\n","Legumes  Txt: NegativeProb: 0.9955717325210571\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0022341832518577576], 'contradiction': [0.9960365891456604], 'neutral': [0.010721100494265556]}, 'neg': {'entailment': [0.002097866265103221], 'contradiction': [0.996501088142395], 'neutral': [0.012111326679587364]}}\n","Animal fat  Txt: NegativeProb: 0.9960365891456604\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.003492100862786174], 'contradiction': [0.9961243271827698], 'neutral': [0.006462199613451958]}, 'neg': {'entailment': [0.0016131260199472308], 'contradiction': [0.99679034948349], 'neutral': [0.0028351470828056335]}}\n","Fats  Txt: NegativeProb: 0.9961243271827698\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0017985482700169086], 'contradiction': [0.9952946305274963], 'neutral': [0.008243760094046593]}, 'neg': {'entailment': [0.0012560408795252442], 'contradiction': [0.9968252182006836], 'neutral': [0.0026369986589998007]}}\n","Sugars  Txt: NegativeProb: 0.9952946305274963\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.005994574166834354], 'contradiction': [0.9946094751358032], 'neutral': [0.008582600392401218]}, 'neg': {'entailment': [0.002171868924051523], 'contradiction': [0.9967665672302246], 'neutral': [0.003684994066134095]}}\n","Carb  Txt: NegativeProb: 0.9946094751358032\n","The Mediterranean and DASH diets are both associated with less cognitive decline. A different approach has been to incorporate elements of both of these diets into one known as the MIND diet. These diets are generally low in saturated fats while providing a good source of carbohydrates, mainly those that help stabilize blood sugar and insulin levels. Raised blood sugar levels over a long time, can damage nerves and cause memory problems if they are not managed. Nutritional factors associated with the proposed diets for reducing dementia risk include unsaturated fatty acids, vitamin E, vitamin C, flavonoids, vitamin B, and vitamin D.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.2192986011505127], 'contradiction': [0.6909547448158264], 'neutral': [0.512422502040863]}, 'neg': {'entailment': [0.21820878982543945], 'contradiction': [0.928823709487915], 'neutral': [0.3897104263305664]}}\n","Vegetable  Txt: PositiveProb: 0.928823709487915\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.3391292989253998], 'contradiction': [0.5767639875411987], 'neutral': [0.735863447189331]}, 'neg': {'entailment': [0.1055876687169075], 'contradiction': [0.951266348361969], 'neutral': [0.22680272161960602]}}\n","Mediterranean Diet  Txt: PositiveProb: 0.951266348361969\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.26473450660705566], 'contradiction': [0.6337127685546875], 'neutral': [0.5368591547012329]}, 'neg': {'entailment': [0.10052066296339035], 'contradiction': [0.9209410548210144], 'neutral': [0.21548669040203094]}}\n","Fruit  Txt: PositiveProb: 0.9209410548210144\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.16423647105693817], 'contradiction': [0.7628535628318787], 'neutral': [0.45027825236320496]}, 'neg': {'entailment': [0.20386627316474915], 'contradiction': [0.9359805583953857], 'neutral': [0.3673745393753052]}}\n","Seaweed  Txt: PositiveProb: 0.9359805583953857\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.05199467018246651], 'contradiction': [0.8413326144218445], 'neutral': [0.34729257225990295]}, 'neg': {'entailment': [0.06929914653301239], 'contradiction': [0.9499945640563965], 'neutral': [0.2196773737668991]}}\n","Bean  Txt: NegativeProb: 0.8413326144218445\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.2924730181694031], 'contradiction': [0.7169483304023743], 'neutral': [0.6112660765647888]}, 'neg': {'entailment': [0.13267609477043152], 'contradiction': [0.9269381761550903], 'neutral': [0.2646715044975281]}}\n","Legume  Txt: PositiveProb: 0.9269381761550903\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.297648161649704], 'contradiction': [0.6679867506027222], 'neutral': [0.5950794816017151]}, 'neg': {'entailment': [0.10942122340202332], 'contradiction': [0.951795756816864], 'neutral': [0.18834111094474792]}}\n","Nut  Txt: PositiveProb: 0.951795756816864\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.14912135899066925], 'contradiction': [0.8052108287811279], 'neutral': [0.5147606134414673]}, 'neg': {'entailment': [0.14448928833007812], 'contradiction': [0.9299356341362], 'neutral': [0.23410725593566895]}}\n","Fish  Txt: NegativeProb: 0.8052108287811279\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.03365947678685188], 'contradiction': [0.8116711974143982], 'neutral': [0.3508574962615967]}, 'neg': {'entailment': [0.09983669221401215], 'contradiction': [0.9387567043304443], 'neutral': [0.23343414068222046]}}\n","Beans  Txt: NegativeProb: 0.8116711974143982\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.2972264885902405], 'contradiction': [0.6415805816650391], 'neutral': [0.5522031188011169]}, 'neg': {'entailment': [0.09970832616090775], 'contradiction': [0.9299476742744446], 'neutral': [0.2115844339132309]}}\n","Fruits  Txt: PositiveProb: 0.9299476742744446\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.15852287411689758], 'contradiction': [0.8009884357452393], 'neutral': [0.514318585395813]}, 'neg': {'entailment': [0.13401903212070465], 'contradiction': [0.9152156710624695], 'neutral': [0.31831517815589905]}}\n","Soy  Txt: NegativeProb: 0.8009884357452393\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.16397583484649658], 'contradiction': [0.782429575920105], 'neutral': [0.38723668456077576]}, 'neg': {'entailment': [0.18870477378368378], 'contradiction': [0.8925166726112366], 'neutral': [0.2331586629152298]}}\n","Oil  Txt: PositiveProb: 0.8925166726112366\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.21558524668216705], 'contradiction': [0.7618647813796997], 'neutral': [0.5830479860305786]}, 'neg': {'entailment': [0.19165968894958496], 'contradiction': [0.9147663712501526], 'neutral': [0.37668731808662415]}}\n","Coffee  Txt: PositiveProb: 0.9147663712501526\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.29468390345573425], 'contradiction': [0.6855543255805969], 'neutral': [0.6170573234558105]}, 'neg': {'entailment': [0.14531080424785614], 'contradiction': [0.9436058402061462], 'neutral': [0.20917361974716187]}}\n","Spread  Txt: PositiveProb: 0.9436058402061462\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.1658666580915451], 'contradiction': [0.6937962174415588], 'neutral': [0.48388147354125977]}, 'neg': {'entailment': [0.2091042399406433], 'contradiction': [0.9236189126968384], 'neutral': [0.29453977942466736]}}\n","Animal fats  Txt: PositiveProb: 0.9236189126968384\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.242811918258667], 'contradiction': [0.767084538936615], 'neutral': [0.45775699615478516]}, 'neg': {'entailment': [0.1459604799747467], 'contradiction': [0.9385657906532288], 'neutral': [0.2874217629432678]}}\n","Oils  Txt: PositiveProb: 0.9385657906532288\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.23318389058113098], 'contradiction': [0.6745738983154297], 'neutral': [0.5635549426078796]}, 'neg': {'entailment': [0.16028450429439545], 'contradiction': [0.9071832299232483], 'neutral': [0.32102149724960327]}}\n","Vegetables  Txt: PositiveProb: 0.9071832299232483\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.10025564581155777], 'contradiction': [0.7542638778686523], 'neutral': [0.42859697341918945]}, 'neg': {'entailment': [0.18794286251068115], 'contradiction': [0.915205180644989], 'neutral': [0.28912320733070374]}}\n","Nuts  Txt: PositiveProb: 0.915205180644989\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.20758309960365295], 'contradiction': [0.798633337020874], 'neutral': [0.3688807785511017]}, 'neg': {'entailment': [0.12514887750148773], 'contradiction': [0.9477936625480652], 'neutral': [0.2751227915287018]}}\n","Meats  Txt: PositiveProb: 0.9477936625480652\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.2762239873409271], 'contradiction': [0.7940434217453003], 'neutral': [0.3179114758968353]}, 'neg': {'entailment': [0.20919813215732574], 'contradiction': [0.9160451889038086], 'neutral': [0.40672504901885986]}}\n","Alcoholic beverages  Txt: PositiveProb: 0.9160451889038086\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.344736784696579], 'contradiction': [0.6696285009384155], 'neutral': [0.6500592827796936]}, 'neg': {'entailment': [0.18593594431877136], 'contradiction': [0.9147403240203857], 'neutral': [0.3144998550415039]}}\n","Sugar  Txt: PositiveProb: 0.9147403240203857\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.26775550842285156], 'contradiction': [0.7454773783683777], 'neutral': [0.5359850525856018]}, 'neg': {'entailment': [0.16796627640724182], 'contradiction': [0.9234278798103333], 'neutral': [0.29734042286872864]}}\n","Proteins  Txt: PositiveProb: 0.9234278798103333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.3127232789993286], 'contradiction': [0.6390754580497742], 'neutral': [0.6583474278450012]}, 'neg': {'entailment': [0.19387418031692505], 'contradiction': [0.9192711710929871], 'neutral': [0.2778518497943878]}}\n","Protein  Txt: PositiveProb: 0.9192711710929871\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.09779926389455795], 'contradiction': [0.8924843668937683], 'neutral': [0.2059447318315506]}, 'neg': {'entailment': [0.07911234349012375], 'contradiction': [0.9548911452293396], 'neutral': [0.13735568523406982]}}\n","Roe  Txt: NegativeProb: 0.8924843668937683\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.24450130760669708], 'contradiction': [0.7515552043914795], 'neutral': [0.5347455143928528]}, 'neg': {'entailment': [0.14206013083457947], 'contradiction': [0.9420310258865356], 'neutral': [0.29869309067726135]}}\n","Ovis  Txt: PositiveProb: 0.9420310258865356\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.2432582527399063], 'contradiction': [0.6274952292442322], 'neutral': [0.6033212542533875]}, 'neg': {'entailment': [0.13158056139945984], 'contradiction': [0.9208253026008606], 'neutral': [0.23463787138462067]}}\n","Citrus  Txt: PositiveProb: 0.9208253026008606\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.20908001065254211], 'contradiction': [0.6720809936523438], 'neutral': [0.5683537721633911]}, 'neg': {'entailment': [0.16417931020259857], 'contradiction': [0.8928425908088684], 'neutral': [0.2804987132549286]}}\n","Mushroom  Txt: PositiveProb: 0.8928425908088684\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.2965580224990845], 'contradiction': [0.7491902112960815], 'neutral': [0.5849606990814209]}, 'neg': {'entailment': [0.13708026707172394], 'contradiction': [0.9332113862037659], 'neutral': [0.2870105504989624]}}\n","Tea  Txt: PositiveProb: 0.9332113862037659\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.23691673576831818], 'contradiction': [0.7581177949905396], 'neutral': [0.407416969537735]}, 'neg': {'entailment': [0.2596345543861389], 'contradiction': [0.9083306789398193], 'neutral': [0.3847595751285553]}}\n","Mushrooms  Txt: PositiveProb: 0.9083306789398193\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.2675170302391052], 'contradiction': [0.7750452756881714], 'neutral': [0.3786967992782593]}, 'neg': {'entailment': [0.26785755157470703], 'contradiction': [0.9187478423118591], 'neutral': [0.44266414642333984]}}\n","Fats and oils  Txt: PositiveProb: 0.9187478423118591\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.26272496581077576], 'contradiction': [0.7586479783058167], 'neutral': [0.3653016686439514]}, 'neg': {'entailment': [0.26503437757492065], 'contradiction': [0.9043396711349487], 'neutral': [0.2429933249950409]}}\n","Fat  Txt: PositiveProb: 0.9043396711349487\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.24359901249408722], 'contradiction': [0.7803657650947571], 'neutral': [0.405538409948349]}, 'neg': {'entailment': [0.27926185727119446], 'contradiction': [0.9025469422340393], 'neutral': [0.5084794759750366]}}\n","Beverages  Txt: PositiveProb: 0.9025469422340393\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.04609772190451622], 'contradiction': [0.807715892791748], 'neutral': [0.4448820948600769]}, 'neg': {'entailment': [0.09200714528560638], 'contradiction': [0.8984880447387695], 'neutral': [0.2581452429294586]}}\n","Meat  Txt: NegativeProb: 0.807715892791748\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.31371253728866577], 'contradiction': [0.7057729959487915], 'neutral': [0.5512133836746216]}, 'neg': {'entailment': [0.13876639306545258], 'contradiction': [0.9234333634376526], 'neutral': [0.2914910912513733]}}\n","Legumes  Txt: PositiveProb: 0.9234333634376526\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.09333814680576324], 'contradiction': [0.8372074961662292], 'neutral': [0.44210389256477356]}, 'neg': {'entailment': [0.15169894695281982], 'contradiction': [0.9078831672668457], 'neutral': [0.2369675636291504]}}\n","Animal fat  Txt: NegativeProb: 0.8372074961662292\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.404694139957428], 'contradiction': [0.761569082736969], 'neutral': [0.4742294251918793]}, 'neg': {'entailment': [0.23240232467651367], 'contradiction': [0.9254198670387268], 'neutral': [0.21391375362873077]}}\n","Fats  Txt: PositiveProb: 0.9254198670387268\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.3059844970703125], 'contradiction': [0.6608996987342834], 'neutral': [0.5680807828903198]}, 'neg': {'entailment': [0.32428812980651855], 'contradiction': [0.9186159372329712], 'neutral': [0.43719035387039185]}}\n","Sugars  Txt: PositiveProb: 0.9186159372329712\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.35236963629722595], 'contradiction': [0.668569803237915], 'neutral': [0.5404336452484131]}, 'neg': {'entailment': [0.1848922222852707], 'contradiction': [0.9110147953033447], 'neutral': [0.22757728397846222]}}\n","Carb  Txt: PositiveProb: 0.9110147953033447\n","Nutritional factors that may increase risk of gallstones include constipation; eating fewer meals per day; low intake of the nutrients folate, magnesium, calcium, and vitamin C; low fluid consumption; and, at least for men, a high intake of carbohydrate, a high glycemic load, and high glycemic index diet. Wine and whole-grained bread may decrease the risk of gallstones.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002232092432677746], 'contradiction': [0.9939672946929932], 'neutral': [0.11970897763967514]}, 'neg': {'entailment': [0.0018370610196143389], 'contradiction': [0.9640719890594482], 'neutral': [0.07259117066860199]}}\n","Vegetable  Txt: NegativeProb: 0.9939672946929932\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0010965308174490929], 'contradiction': [0.9929726123809814], 'neutral': [0.2523210346698761]}, 'neg': {'entailment': [0.0010104919783771038], 'contradiction': [0.9497759342193604], 'neutral': [0.2200220823287964]}}\n","Mediterranean Diet  Txt: NegativeProb: 0.9929726123809814\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0013967420672997832], 'contradiction': [0.9905028343200684], 'neutral': [0.16065450012683868]}, 'neg': {'entailment': [0.0011544587323442101], 'contradiction': [0.9490950703620911], 'neutral': [0.11139008402824402]}}\n","Fruit  Txt: NegativeProb: 0.9905028343200684\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001058473833836615], 'contradiction': [0.9935309290885925], 'neutral': [0.059314604848623276]}, 'neg': {'entailment': [0.0013210158795118332], 'contradiction': [0.9714340567588806], 'neutral': [0.0512801855802536]}}\n","Seaweed  Txt: NegativeProb: 0.9935309290885925\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0012275450862944126], 'contradiction': [0.9958178400993347], 'neutral': [0.08069559186697006]}, 'neg': {'entailment': [0.0018329424783587456], 'contradiction': [0.963405430316925], 'neutral': [0.08367625623941422]}}\n","Bean  Txt: NegativeProb: 0.9958178400993347\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002902858192101121], 'contradiction': [0.9932333827018738], 'neutral': [0.054492685943841934]}, 'neg': {'entailment': [0.0035210484638810158], 'contradiction': [0.9724470376968384], 'neutral': [0.07066761702299118]}}\n","Legume  Txt: NegativeProb: 0.9932333827018738\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.010197889059782028], 'contradiction': [0.9962711334228516], 'neutral': [0.1164008378982544]}, 'neg': {'entailment': [0.052814632654190063], 'contradiction': [0.9562370777130127], 'neutral': [0.27627068758010864]}}\n","Nut  Txt: NegativeProb: 0.9962711334228516\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0012225606478750706], 'contradiction': [0.9951139092445374], 'neutral': [0.055790700018405914]}, 'neg': {'entailment': [0.001171930693089962], 'contradiction': [0.9691753387451172], 'neutral': [0.07765301316976547]}}\n","Fish  Txt: NegativeProb: 0.9951139092445374\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0010963911190629005], 'contradiction': [0.9957796335220337], 'neutral': [0.07597934454679489]}, 'neg': {'entailment': [0.0012748833978548646], 'contradiction': [0.972170889377594], 'neutral': [0.08037518709897995]}}\n","Beans  Txt: NegativeProb: 0.9957796335220337\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001976321218535304], 'contradiction': [0.989514946937561], 'neutral': [0.12351177632808685]}, 'neg': {'entailment': [0.0014903406845405698], 'contradiction': [0.963007390499115], 'neutral': [0.06589191406965256]}}\n","Fruits  Txt: NegativeProb: 0.989514946937561\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0011312636779621243], 'contradiction': [0.9955170750617981], 'neutral': [0.12591183185577393]}, 'neg': {'entailment': [0.0012006134493276477], 'contradiction': [0.9577330946922302], 'neutral': [0.12720857560634613]}}\n","Soy  Txt: NegativeProb: 0.9955170750617981\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0017170760547742248], 'contradiction': [0.9955683350563049], 'neutral': [0.12500138580799103]}, 'neg': {'entailment': [0.0027441096026450396], 'contradiction': [0.95330411195755], 'neutral': [0.16218112409114838]}}\n","Oil  Txt: NegativeProb: 0.9955683350563049\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001582912402227521], 'contradiction': [0.993755578994751], 'neutral': [0.07987851649522781]}, 'neg': {'entailment': [0.0020608657505363226], 'contradiction': [0.9557618498802185], 'neutral': [0.08064395934343338]}}\n","Coffee  Txt: NegativeProb: 0.993755578994751\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002567111514508724], 'contradiction': [0.9964941143989563], 'neutral': [0.10409603267908096]}, 'neg': {'entailment': [0.005964312702417374], 'contradiction': [0.9633145928382874], 'neutral': [0.2699509561061859]}}\n","Spread  Txt: NegativeProb: 0.9964941143989563\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0012928672367706895], 'contradiction': [0.9945229291915894], 'neutral': [0.08279335498809814]}, 'neg': {'entailment': [0.0013154858024790883], 'contradiction': [0.9644492864608765], 'neutral': [0.06350386142730713]}}\n","Animal fats  Txt: NegativeProb: 0.9945229291915894\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0022063024807721376], 'contradiction': [0.9948859810829163], 'neutral': [0.08592357486486435]}, 'neg': {'entailment': [0.0018289616564288735], 'contradiction': [0.9719924926757812], 'neutral': [0.08495678752660751]}}\n","Oils  Txt: NegativeProb: 0.9948859810829163\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002069191075861454], 'contradiction': [0.9918373227119446], 'neutral': [0.14671455323696136]}, 'neg': {'entailment': [0.001571465516462922], 'contradiction': [0.9496009945869446], 'neutral': [0.08438446372747421]}}\n","Vegetables  Txt: NegativeProb: 0.9918373227119446\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0011728660902008414], 'contradiction': [0.9951523542404175], 'neutral': [0.06184326857328415]}, 'neg': {'entailment': [0.0012471064692363143], 'contradiction': [0.9769960045814514], 'neutral': [0.03514832630753517]}}\n","Nuts  Txt: NegativeProb: 0.9951523542404175\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002956818789243698], 'contradiction': [0.9963995218276978], 'neutral': [0.050974711775779724]}, 'neg': {'entailment': [0.0040892562828958035], 'contradiction': [0.9767740964889526], 'neutral': [0.07220788300037384]}}\n","Meats  Txt: NegativeProb: 0.9963995218276978\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0028542024083435535], 'contradiction': [0.9925193786621094], 'neutral': [0.06723589450120926]}, 'neg': {'entailment': [0.0022188557777553797], 'contradiction': [0.978582501411438], 'neutral': [0.04238447546958923]}}\n","Alcoholic beverages  Txt: NegativeProb: 0.9925193786621094\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002140194643288851], 'contradiction': [0.9949765205383301], 'neutral': [0.12127238512039185]}, 'neg': {'entailment': [0.0028681624680757523], 'contradiction': [0.9495286345481873], 'neutral': [0.12097489833831787]}}\n","Sugar  Txt: NegativeProb: 0.9949765205383301\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.004817754030227661], 'contradiction': [0.9955756664276123], 'neutral': [0.0733882412314415]}, 'neg': {'entailment': [0.0034884591586887836], 'contradiction': [0.9801223278045654], 'neutral': [0.06298570334911346]}}\n","Proteins  Txt: NegativeProb: 0.9955756664276123\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0023908924777060747], 'contradiction': [0.9933306574821472], 'neutral': [0.16609498858451843]}, 'neg': {'entailment': [0.004438341129571199], 'contradiction': [0.9419712424278259], 'neutral': [0.10154762864112854]}}\n","Protein  Txt: NegativeProb: 0.9933306574821472\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001316532026976347], 'contradiction': [0.9960442781448364], 'neutral': [0.049229033291339874]}, 'neg': {'entailment': [0.002041855361312628], 'contradiction': [0.9717093110084534], 'neutral': [0.13155996799468994]}}\n","Roe  Txt: NegativeProb: 0.9960442781448364\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0024342599790543318], 'contradiction': [0.9962054491043091], 'neutral': [0.05483665689826012]}, 'neg': {'entailment': [0.004560563713312149], 'contradiction': [0.9670428037643433], 'neutral': [0.1756163239479065]}}\n","Ovis  Txt: NegativeProb: 0.9962054491043091\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0013022696366533637], 'contradiction': [0.9943168759346008], 'neutral': [0.10056857019662857]}, 'neg': {'entailment': [0.0015171441482380033], 'contradiction': [0.967743992805481], 'neutral': [0.0869503766298294]}}\n","Citrus  Txt: NegativeProb: 0.9943168759346008\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0011845233384519815], 'contradiction': [0.9947450160980225], 'neutral': [0.05987488850951195]}, 'neg': {'entailment': [0.0013112202286720276], 'contradiction': [0.9748387336730957], 'neutral': [0.05965295806527138]}}\n","Mushroom  Txt: NegativeProb: 0.9947450160980225\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0011559196282178164], 'contradiction': [0.9946243166923523], 'neutral': [0.07708951085805893]}, 'neg': {'entailment': [0.0014020871603861451], 'contradiction': [0.9693789482116699], 'neutral': [0.06826408207416534]}}\n","Tea  Txt: NegativeProb: 0.9946243166923523\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0010082436492666602], 'contradiction': [0.9926695823669434], 'neutral': [0.08180460333824158]}, 'neg': {'entailment': [0.0011046933941543102], 'contradiction': [0.9638850688934326], 'neutral': [0.06220323592424393]}}\n","Mushrooms  Txt: NegativeProb: 0.9926695823669434\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.003964765928685665], 'contradiction': [0.9952943921089172], 'neutral': [0.038374993950128555]}, 'neg': {'entailment': [0.007313594687730074], 'contradiction': [0.9764947891235352], 'neutral': [0.04461013153195381]}}\n","Fats and oils  Txt: NegativeProb: 0.9952943921089172\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0020716958679258823], 'contradiction': [0.9965419173240662], 'neutral': [0.03400527313351631]}, 'neg': {'entailment': [0.005557054188102484], 'contradiction': [0.9710174202919006], 'neutral': [0.06916941702365875]}}\n","Fat  Txt: NegativeProb: 0.9965419173240662\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0188242606818676], 'contradiction': [0.9949800372123718], 'neutral': [0.1721571832895279]}, 'neg': {'entailment': [0.0506080724298954], 'contradiction': [0.9537827372550964], 'neutral': [0.13973890244960785]}}\n","Beverages  Txt: NegativeProb: 0.9949800372123718\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0011147587792947888], 'contradiction': [0.9950083494186401], 'neutral': [0.0940910279750824]}, 'neg': {'entailment': [0.0022409313824027777], 'contradiction': [0.9437469244003296], 'neutral': [0.20219090580940247]}}\n","Meat  Txt: NegativeProb: 0.9950083494186401\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0029287186916917562], 'contradiction': [0.9895877838134766], 'neutral': [0.0782640352845192]}, 'neg': {'entailment': [0.002616561483591795], 'contradiction': [0.9672282338142395], 'neutral': [0.07794966548681259]}}\n","Legumes  Txt: NegativeProb: 0.9895877838134766\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0012784047285094857], 'contradiction': [0.9948955178260803], 'neutral': [0.07323101162910461]}, 'neg': {'entailment': [0.0012713370379060507], 'contradiction': [0.9685972332954407], 'neutral': [0.06364449858665466]}}\n","Animal fat  Txt: NegativeProb: 0.9948955178260803\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.004310582764446735], 'contradiction': [0.9961861968040466], 'neutral': [0.04971904307603836]}, 'neg': {'entailment': [0.004370323847979307], 'contradiction': [0.9727160334587097], 'neutral': [0.05565013363957405]}}\n","Fats  Txt: NegativeProb: 0.9961861968040466\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001852675573900342], 'contradiction': [0.9921061992645264], 'neutral': [0.15777373313903809]}, 'neg': {'entailment': [0.0015396290691569448], 'contradiction': [0.9479021430015564], 'neutral': [0.12953145802021027]}}\n","Sugars  Txt: NegativeProb: 0.9921061992645264\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.005608266219496727], 'contradiction': [0.9928598999977112], 'neutral': [0.3389301598072052]}, 'neg': {'entailment': [0.03399857506155968], 'contradiction': [0.90986168384552], 'neutral': [0.27737927436828613]}}\n","Carb  Txt: NegativeProb: 0.9928598999977112\n","Fish may lower prostate-cancer deaths, but does not appear to affect occurrence. Some evidence supports lower rates of prostate cancer with a vegetarian diet, lycopene, selenium cruciferous vegetables, soy, beans and/or other legumes.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.8322328329086304], 'contradiction': [0.5433987975120544], 'neutral': [0.9860628247261047]}, 'neg': {'entailment': [0.1122170239686966], 'contradiction': [0.9959934949874878], 'neutral': [0.16885985434055328]}}\n","Vegetable  Txt: PositiveProb: 0.9959934949874878\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.16601207852363586], 'contradiction': [0.06397073715925217], 'neutral': [0.9823414087295532]}, 'neg': {'entailment': [0.03119061514735222], 'contradiction': [0.994304358959198], 'neutral': [0.6409112215042114]}}\n","Mediterranean Diet  Txt: PositiveProb: 0.994304358959198\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.06412480026483536], 'contradiction': [0.9524102807044983], 'neutral': [0.886482298374176]}, 'neg': {'entailment': [0.012531127780675888], 'contradiction': [0.9944705963134766], 'neutral': [0.15131939947605133]}}\n","Fruit  Txt: NegativeProb: 0.9524102807044983\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.19424070417881012], 'contradiction': [0.878201961517334], 'neutral': [0.8294694423675537]}, 'neg': {'entailment': [0.018822703510522842], 'contradiction': [0.9963884353637695], 'neutral': [0.09708663821220398]}}\n","Seaweed  Txt: NegativeProb: 0.878201961517334\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.33551931381225586], 'contradiction': [0.8582971692085266], 'neutral': [0.8912180066108704]}, 'neg': {'entailment': [0.028083933517336845], 'contradiction': [0.9644415974617004], 'neutral': [0.3492084741592407]}}\n","Bean  Txt: PositiveProb: 0.9644415974617004\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.6753607988357544], 'contradiction': [0.2997424304485321], 'neutral': [0.9882366061210632]}, 'neg': {'entailment': [0.3142164945602417], 'contradiction': [0.9859841465950012], 'neutral': [0.4183575212955475]}}\n","Legume  Txt: PositiveProb: 0.9859841465950012\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.21832376718521118], 'contradiction': [0.8433585166931152], 'neutral': [0.7002429962158203]}, 'neg': {'entailment': [0.04160686209797859], 'contradiction': [0.9955165982246399], 'neutral': [0.11192946881055832]}}\n","Nut  Txt: NegativeProb: 0.8433585166931152\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.6240472793579102], 'contradiction': [0.6800132393836975], 'neutral': [0.9501248002052307]}, 'neg': {'entailment': [0.3094082772731781], 'contradiction': [0.9964308738708496], 'neutral': [0.13319307565689087]}}\n","Fish  Txt: PositiveProb: 0.9964308738708496\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.22351506352424622], 'contradiction': [0.6860716938972473], 'neutral': [0.9565390944480896]}, 'neg': {'entailment': [0.015078336000442505], 'contradiction': [0.9687795639038086], 'neutral': [0.4440750777721405]}}\n","Beans  Txt: PositiveProb: 0.9687795639038086\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.31857264041900635], 'contradiction': [0.907458484172821], 'neutral': [0.8663803935050964]}, 'neg': {'entailment': [0.023747125640511513], 'contradiction': [0.993039071559906], 'neutral': [0.06876560300588608]}}\n","Fruits  Txt: NegativeProb: 0.907458484172821\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.3342497646808624], 'contradiction': [0.19757477939128876], 'neutral': [0.99440997838974]}, 'neg': {'entailment': [0.0906195342540741], 'contradiction': [0.9638424515724182], 'neutral': [0.583706796169281]}}\n","Soy  Txt: PositiveProb: 0.9638424515724182\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.1904982030391693], 'contradiction': [0.9724251627922058], 'neutral': [0.45323219895362854]}, 'neg': {'entailment': [0.23632441461086273], 'contradiction': [0.99467933177948], 'neutral': [0.12492727488279343]}}\n","Oil  Txt: NegativeProb: 0.9724251627922058\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0032873162999749184], 'contradiction': [0.9980138540267944], 'neutral': [0.26008695363998413]}, 'neg': {'entailment': [0.0009415240492671728], 'contradiction': [0.9985234141349792], 'neutral': [0.0027945886831730604]}}\n","Coffee  Txt: NegativeProb: 0.9980138540267944\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.22981704771518707], 'contradiction': [0.6743963360786438], 'neutral': [0.9027411341667175]}, 'neg': {'entailment': [0.12554797530174255], 'contradiction': [0.9508553147315979], 'neutral': [0.7765165567398071]}}\n","Spread  Txt: PositiveProb: 0.9508553147315979\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.6545026302337646], 'contradiction': [0.7015714049339294], 'neutral': [0.6239919662475586]}, 'neg': {'entailment': [0.24565482139587402], 'contradiction': [0.996374785900116], 'neutral': [0.068978451192379]}}\n","Animal fats  Txt: PositiveProb: 0.996374785900116\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.16978859901428223], 'contradiction': [0.9191305637359619], 'neutral': [0.8480198979377747]}, 'neg': {'entailment': [0.01558967400342226], 'contradiction': [0.9936909675598145], 'neutral': [0.15284965932369232]}}\n","Oils  Txt: NegativeProb: 0.9191305637359619\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.7989864349365234], 'contradiction': [0.4507772624492645], 'neutral': [0.9866248369216919]}, 'neg': {'entailment': [0.07563265413045883], 'contradiction': [0.9961962699890137], 'neutral': [0.14492066204547882]}}\n","Vegetables  Txt: PositiveProb: 0.9961962699890137\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.007205495610833168], 'contradiction': [0.9962723255157471], 'neutral': [0.1360766887664795]}, 'neg': {'entailment': [0.0014996373793110251], 'contradiction': [0.9969080090522766], 'neutral': [0.010743945837020874]}}\n","Nuts  Txt: NegativeProb: 0.9962723255157471\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.21970145404338837], 'contradiction': [0.9840112328529358], 'neutral': [0.1565091758966446]}, 'neg': {'entailment': [0.06479600071907043], 'contradiction': [0.9956721067428589], 'neutral': [0.04859071969985962]}}\n","Meats  Txt: NegativeProb: 0.9840112328529358\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.08247272670269012], 'contradiction': [0.9949786067008972], 'neutral': [0.1454572230577469]}, 'neg': {'entailment': [0.025312107056379318], 'contradiction': [0.9954453706741333], 'neutral': [0.01805528998374939]}}\n","Alcoholic beverages  Txt: NegativeProb: 0.9949786067008972\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.011731152422726154], 'contradiction': [0.9893423318862915], 'neutral': [0.7872276306152344]}, 'neg': {'entailment': [0.02807675115764141], 'contradiction': [0.9925597906112671], 'neutral': [0.15966129302978516]}}\n","Sugar  Txt: NegativeProb: 0.9893423318862915\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.5535658001899719], 'contradiction': [0.6680129766464233], 'neutral': [0.9282375574111938]}, 'neg': {'entailment': [0.20898868143558502], 'contradiction': [0.994273841381073], 'neutral': [0.21163515746593475]}}\n","Proteins  Txt: PositiveProb: 0.994273841381073\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.40265747904777527], 'contradiction': [0.8040122389793396], 'neutral': [0.8660579919815063]}, 'neg': {'entailment': [0.1181316003203392], 'contradiction': [0.9912838935852051], 'neutral': [0.1221144050359726]}}\n","Protein  Txt: PositiveProb: 0.9912838935852051\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.07205913960933685], 'contradiction': [0.9246996641159058], 'neutral': [0.7232230305671692]}, 'neg': {'entailment': [0.008107835426926613], 'contradiction': [0.9927394986152649], 'neutral': [0.3301945924758911]}}\n","Roe  Txt: NegativeProb: 0.9246996641159058\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.3433612883090973], 'contradiction': [0.914962112903595], 'neutral': [0.8446113467216492]}, 'neg': {'entailment': [0.1720128059387207], 'contradiction': [0.9806883335113525], 'neutral': [0.7751981616020203]}}\n","Ovis  Txt: NegativeProb: 0.914962112903595\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.6002641916275024], 'contradiction': [0.7750130295753479], 'neutral': [0.9153258204460144]}, 'neg': {'entailment': [0.11865994334220886], 'contradiction': [0.9871677756309509], 'neutral': [0.22447043657302856]}}\n","Citrus  Txt: PositiveProb: 0.9871677756309509\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.01313014980405569], 'contradiction': [0.9935697317123413], 'neutral': [0.6184333562850952]}, 'neg': {'entailment': [0.0015256929909810424], 'contradiction': [0.996609091758728], 'neutral': [0.023777928203344345]}}\n","Mushroom  Txt: NegativeProb: 0.9935697317123413\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.03509863093495369], 'contradiction': [0.9901963472366333], 'neutral': [0.15849833190441132]}, 'neg': {'entailment': [0.0025261223781853914], 'contradiction': [0.9968279004096985], 'neutral': [0.019159574061632156]}}\n","Tea  Txt: NegativeProb: 0.9901963472366333\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.012431284412741661], 'contradiction': [0.977111279964447], 'neutral': [0.7967910170555115]}, 'neg': {'entailment': [0.0017244876362383366], 'contradiction': [0.9960232973098755], 'neutral': [0.04804842546582222]}}\n","Mushrooms  Txt: NegativeProb: 0.977111279964447\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.7500558495521545], 'contradiction': [0.5663876533508301], 'neutral': [0.9155944585800171]}, 'neg': {'entailment': [0.22398996353149414], 'contradiction': [0.9962818026542664], 'neutral': [0.1835082322359085]}}\n","Fats and oils  Txt: PositiveProb: 0.9962818026542664\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.15526509284973145], 'contradiction': [0.9126260876655579], 'neutral': [0.7275205254554749]}, 'neg': {'entailment': [0.08510638028383255], 'contradiction': [0.9762758016586304], 'neutral': [0.5110523700714111]}}\n","Fat  Txt: NegativeProb: 0.9126260876655579\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.5746453404426575], 'contradiction': [0.4238130450248718], 'neutral': [0.9441128373146057]}, 'neg': {'entailment': [0.25282779335975647], 'contradiction': [0.9938633441925049], 'neutral': [0.21901865303516388]}}\n","Beverages  Txt: PositiveProb: 0.9938633441925049\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.06049942597746849], 'contradiction': [0.9879789352416992], 'neutral': [0.13483315706253052]}, 'neg': {'entailment': [0.13988220691680908], 'contradiction': [0.994814932346344], 'neutral': [0.1826903373003006]}}\n","Meat  Txt: NegativeProb: 0.9879789352416992\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.5810980796813965], 'contradiction': [0.27107879519462585], 'neutral': [0.9881949424743652]}, 'neg': {'entailment': [0.19721795618534088], 'contradiction': [0.9932957291603088], 'neutral': [0.27376285195350647]}}\n","Legumes  Txt: PositiveProb: 0.9932957291603088\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.6571793556213379], 'contradiction': [0.8417717218399048], 'neutral': [0.5629763007164001]}, 'neg': {'entailment': [0.25275567173957825], 'contradiction': [0.9951025247573853], 'neutral': [0.062099285423755646]}}\n","Animal fat  Txt: NegativeProb: 0.8417717218399048\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.5279622077941895], 'contradiction': [0.8611729145050049], 'neutral': [0.8005781173706055]}, 'neg': {'entailment': [0.053297191858291626], 'contradiction': [0.995880126953125], 'neutral': [0.06858602911233902]}}\n","Fats  Txt: NegativeProb: 0.8611729145050049\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.3178423047065735], 'contradiction': [0.34136152267456055], 'neutral': [0.9904308915138245]}, 'neg': {'entailment': [0.031385231763124466], 'contradiction': [0.9908125996589661], 'neutral': [0.4021705985069275]}}\n","Sugars  Txt: PositiveProb: 0.9908125996589661\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.8063501119613647], 'contradiction': [0.5258653163909912], 'neutral': [0.8531040549278259]}, 'neg': {'entailment': [0.3222922086715698], 'contradiction': [0.9929473400115967], 'neutral': [0.1146061047911644]}}\n","Carb  Txt: PositiveProb: 0.9929473400115967\n","While many dietary recommendations have been proposed to reduce cancer risks, the evidence to support them is not definitive. The primary dietary factors that increase risk are obesity and alcohol consumption. Diets low in fruits and vegetables and high in red meat have been implicated but reviews and meta-analyses do not come to a consistent conclusion. A 2014 meta-analysis found no relationship between fruits and vegetables and cancer. Coffee is associated with a reduced risk of liver cancer. Studies have linked excessive consumption of red or processed meat to an increased risk of breast cancer, colon cancer and pancreatic cancer, a phenomenon that could be due to the presence of carcinogens in meats cooked at high temperatures. In 2015 the IARC reported that eating processed meat  and, to a lesser degree, red meat was linked to some cancers.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001442129723727703], 'contradiction': [0.989518940448761], 'neutral': [0.035276710987091064]}, 'neg': {'entailment': [0.0013638376258313656], 'contradiction': [0.9797448515892029], 'neutral': [0.032696306705474854]}}\n","Vegetable  Txt: NegativeProb: 0.989518940448761\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0015215515159070492], 'contradiction': [0.9732681512832642], 'neutral': [0.09954597800970078]}, 'neg': {'entailment': [0.0013366562779992819], 'contradiction': [0.9496238827705383], 'neutral': [0.09153131395578384]}}\n","Mediterranean Diet  Txt: NegativeProb: 0.9732681512832642\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0014725588262081146], 'contradiction': [0.985069215297699], 'neutral': [0.04226493835449219]}, 'neg': {'entailment': [0.0012504970654845238], 'contradiction': [0.9759702682495117], 'neutral': [0.036904364824295044]}}\n","Fruit  Txt: NegativeProb: 0.985069215297699\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0012289475416764617], 'contradiction': [0.9868956208229065], 'neutral': [0.043618347495794296]}, 'neg': {'entailment': [0.0011819155188277364], 'contradiction': [0.9739091992378235], 'neutral': [0.037880558520555496]}}\n","Seaweed  Txt: NegativeProb: 0.9868956208229065\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0013359433505684137], 'contradiction': [0.9902594685554504], 'neutral': [0.02905704639852047]}, 'neg': {'entailment': [0.0012657034676522017], 'contradiction': [0.9839688539505005], 'neutral': [0.02915324829518795]}}\n","Bean  Txt: NegativeProb: 0.9902594685554504\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0016167153371497989], 'contradiction': [0.9912291765213013], 'neutral': [0.030372975394129753]}, 'neg': {'entailment': [0.001459083752706647], 'contradiction': [0.9807496666908264], 'neutral': [0.031384821981191635]}}\n","Legume  Txt: NegativeProb: 0.9912291765213013\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0017947175074368715], 'contradiction': [0.9936981797218323], 'neutral': [0.027174819260835648]}, 'neg': {'entailment': [0.0015062019228935242], 'contradiction': [0.9853411316871643], 'neutral': [0.0326729416847229]}}\n","Nut  Txt: NegativeProb: 0.9936981797218323\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.00150248350109905], 'contradiction': [0.9893450140953064], 'neutral': [0.041714586317539215]}, 'neg': {'entailment': [0.0012856239918619394], 'contradiction': [0.9756948947906494], 'neutral': [0.041010454297065735]}}\n","Fish  Txt: NegativeProb: 0.9893450140953064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0012244072277098894], 'contradiction': [0.9881253242492676], 'neutral': [0.03722652792930603]}, 'neg': {'entailment': [0.0012136587174609303], 'contradiction': [0.9782906770706177], 'neutral': [0.03692355006933212]}}\n","Beans  Txt: NegativeProb: 0.9881253242492676\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001578180119395256], 'contradiction': [0.9854554533958435], 'neutral': [0.03737912327051163]}, 'neg': {'entailment': [0.0014039777452126145], 'contradiction': [0.9797260165214539], 'neutral': [0.0318358838558197]}}\n","Fruits  Txt: NegativeProb: 0.9854554533958435\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0014017364010214806], 'contradiction': [0.9908377528190613], 'neutral': [0.03492638096213341]}, 'neg': {'entailment': [0.0012811989290639758], 'contradiction': [0.9793891310691833], 'neutral': [0.03552652895450592]}}\n","Soy  Txt: NegativeProb: 0.9908377528190613\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0017019094666466117], 'contradiction': [0.9900736808776855], 'neutral': [0.04593747481703758]}, 'neg': {'entailment': [0.0014358051121234894], 'contradiction': [0.9700676798820496], 'neutral': [0.04683101177215576]}}\n","Oil  Txt: NegativeProb: 0.9900736808776855\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0015245976392179728], 'contradiction': [0.9909756779670715], 'neutral': [0.030998047441244125]}, 'neg': {'entailment': [0.0014013476902619004], 'contradiction': [0.9792653322219849], 'neutral': [0.03135668486356735]}}\n","Coffee  Txt: NegativeProb: 0.9909756779670715\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0018831375055015087], 'contradiction': [0.9937455654144287], 'neutral': [0.04295577108860016]}, 'neg': {'entailment': [0.0023726001381874084], 'contradiction': [0.9802432656288147], 'neutral': [0.048818476498126984]}}\n","Spread  Txt: NegativeProb: 0.9937455654144287\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0022635592613369226], 'contradiction': [0.9920855164527893], 'neutral': [0.040230296552181244]}, 'neg': {'entailment': [0.001871889573521912], 'contradiction': [0.9752351641654968], 'neutral': [0.04148272052407265]}}\n","Animal fats  Txt: NegativeProb: 0.9920855164527893\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0017893047770485282], 'contradiction': [0.9911261200904846], 'neutral': [0.03846374899148941]}, 'neg': {'entailment': [0.0015657760668545961], 'contradiction': [0.9768664240837097], 'neutral': [0.03801747411489487]}}\n","Oils  Txt: NegativeProb: 0.9911261200904846\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0013697423273697495], 'contradiction': [0.9879788160324097], 'neutral': [0.04008455574512482]}, 'neg': {'entailment': [0.0012900945730507374], 'contradiction': [0.9784467816352844], 'neutral': [0.03441913053393364]}}\n","Vegetables  Txt: NegativeProb: 0.9879788160324097\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0011411653831601143], 'contradiction': [0.9922850131988525], 'neutral': [0.03158263489603996]}, 'neg': {'entailment': [0.0012195783201605082], 'contradiction': [0.9810551404953003], 'neutral': [0.0291997529566288]}}\n","Nuts  Txt: NegativeProb: 0.9922850131988525\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.003129589604213834], 'contradiction': [0.9922884106636047], 'neutral': [0.04157215356826782]}, 'neg': {'entailment': [0.0019371751695871353], 'contradiction': [0.9751912951469421], 'neutral': [0.04623137414455414]}}\n","Meats  Txt: NegativeProb: 0.9922884106636047\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0016686012968420982], 'contradiction': [0.9922325611114502], 'neutral': [0.02192671038210392]}, 'neg': {'entailment': [0.001666491269133985], 'contradiction': [0.9831514358520508], 'neutral': [0.019934432581067085]}}\n","Alcoholic beverages  Txt: NegativeProb: 0.9922325611114502\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0013157661305740476], 'contradiction': [0.9900133609771729], 'neutral': [0.03772808983922005]}, 'neg': {'entailment': [0.0012470068177208304], 'contradiction': [0.9718082547187805], 'neutral': [0.04088883474469185]}}\n","Sugar  Txt: NegativeProb: 0.9900133609771729\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0023493876215070486], 'contradiction': [0.9918187856674194], 'neutral': [0.03742893785238266]}, 'neg': {'entailment': [0.0016857818700373173], 'contradiction': [0.9766155481338501], 'neutral': [0.0399874746799469]}}\n","Proteins  Txt: NegativeProb: 0.9918187856674194\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002803222741931677], 'contradiction': [0.9934212565422058], 'neutral': [0.03416065499186516]}, 'neg': {'entailment': [0.001788160763680935], 'contradiction': [0.9768965840339661], 'neutral': [0.03976418077945709]}}\n","Protein  Txt: NegativeProb: 0.9934212565422058\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0020076725631952286], 'contradiction': [0.9926643967628479], 'neutral': [0.03449799865484238]}, 'neg': {'entailment': [0.0014030135935172439], 'contradiction': [0.9823143482208252], 'neutral': [0.03060334548354149]}}\n","Roe  Txt: NegativeProb: 0.9926643967628479\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0023938396479934454], 'contradiction': [0.9928003549575806], 'neutral': [0.0364353209733963]}, 'neg': {'entailment': [0.001567013910971582], 'contradiction': [0.9790894389152527], 'neutral': [0.03786883503198624]}}\n","Ovis  Txt: NegativeProb: 0.9928003549575806\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0012707753339782357], 'contradiction': [0.9880337119102478], 'neutral': [0.03563060238957405]}, 'neg': {'entailment': [0.0012267313431948423], 'contradiction': [0.977938175201416], 'neutral': [0.031071553006768227]}}\n","Citrus  Txt: NegativeProb: 0.9880337119102478\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0012876089895144105], 'contradiction': [0.9874390959739685], 'neutral': [0.04359876364469528]}, 'neg': {'entailment': [0.0012361047556623816], 'contradiction': [0.9757053256034851], 'neutral': [0.036706358194351196]}}\n","Mushroom  Txt: NegativeProb: 0.9874390959739685\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0011526047019287944], 'contradiction': [0.9914910197257996], 'neutral': [0.030203182250261307]}, 'neg': {'entailment': [0.0011254001874476671], 'contradiction': [0.982993483543396], 'neutral': [0.029750414192676544]}}\n","Tea  Txt: NegativeProb: 0.9914910197257996\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001134983147494495], 'contradiction': [0.9868037104606628], 'neutral': [0.05114378407597542]}, 'neg': {'entailment': [0.001106789568439126], 'contradiction': [0.9732796549797058], 'neutral': [0.04151935502886772]}}\n","Mushrooms  Txt: NegativeProb: 0.9868037104606628\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0017571344505995512], 'contradiction': [0.9900774955749512], 'neutral': [0.0367119163274765]}, 'neg': {'entailment': [0.0017873154720291495], 'contradiction': [0.9756909012794495], 'neutral': [0.04141288995742798]}}\n","Fats and oils  Txt: NegativeProb: 0.9900774955749512\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0017864542314782739], 'contradiction': [0.9919103980064392], 'neutral': [0.03701277822256088]}, 'neg': {'entailment': [0.001516832271590829], 'contradiction': [0.973548412322998], 'neutral': [0.042496733367443085]}}\n","Fat  Txt: NegativeProb: 0.9919103980064392\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002365531399846077], 'contradiction': [0.9903711676597595], 'neutral': [0.03506718575954437]}, 'neg': {'entailment': [0.0016536950133740902], 'contradiction': [0.9785814881324768], 'neutral': [0.031667560338974]}}\n","Beverages  Txt: NegativeProb: 0.9903711676597595\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002445480553433299], 'contradiction': [0.9926615953445435], 'neutral': [0.04164884611964226]}, 'neg': {'entailment': [0.001554243266582489], 'contradiction': [0.9776195287704468], 'neutral': [0.04342028871178627]}}\n","Meat  Txt: NegativeProb: 0.9926615953445435\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0013730840291827917], 'contradiction': [0.9897757768630981], 'neutral': [0.034447237849235535]}, 'neg': {'entailment': [0.0013976821210235357], 'contradiction': [0.9817882776260376], 'neutral': [0.029540127143263817]}}\n","Legumes  Txt: NegativeProb: 0.9897757768630981\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.001968712778761983], 'contradiction': [0.9924440979957581], 'neutral': [0.03836362808942795]}, 'neg': {'entailment': [0.0015466916374862194], 'contradiction': [0.9744008183479309], 'neutral': [0.03996763005852699]}}\n","Animal fat  Txt: NegativeProb: 0.9924440979957581\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0018202881328761578], 'contradiction': [0.9914994835853577], 'neutral': [0.03403918817639351]}, 'neg': {'entailment': [0.001492122421041131], 'contradiction': [0.9800704121589661], 'neutral': [0.03285226598381996]}}\n","Fats  Txt: NegativeProb: 0.9914994835853577\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0019828900694847107], 'contradiction': [0.987403392791748], 'neutral': [0.036944206804037094]}, 'neg': {'entailment': [0.0014127468457445502], 'contradiction': [0.9750544428825378], 'neutral': [0.03754919394850731]}}\n","Sugars  Txt: NegativeProb: 0.987403392791748\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0031584210228174925], 'contradiction': [0.9923641681671143], 'neutral': [0.03367899730801582]}, 'neg': {'entailment': [0.0016871863044798374], 'contradiction': [0.9782702922821045], 'neutral': [0.0342874675989151]}}\n","Carb  Txt: NegativeProb: 0.9923641681671143\n","In 2016 research into the archives of theSugar Association, the trade association for the sugar industry in the US, had sponsored an influential literature review published in 1965 in the New England Journal of Medicine that downplayed early findings about the role of a diet heavy in sugar in the development of CAD and emphasized the role of fat; that review influenced decades of research funding and guidance on healthy eating.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.006734187714755535], 'contradiction': [0.8675188422203064], 'neutral': [0.6611198782920837]}, 'neg': {'entailment': [0.0012579981703311205], 'contradiction': [0.953464150428772], 'neutral': [0.2405967265367508]}}\n","Vegetable  Txt: NegativeProb: 0.8675188422203064\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002353509422391653], 'contradiction': [0.6990118026733398], 'neutral': [0.6595326662063599]}, 'neg': {'entailment': [0.0008889004238881171], 'contradiction': [0.8164680004119873], 'neutral': [0.3828830420970917]}}\n","Mediterranean Diet  Txt: PositiveProb: 0.8164680004119873\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.012973957695066929], 'contradiction': [0.37009286880493164], 'neutral': [0.8978890776634216]}, 'neg': {'entailment': [0.004116217605769634], 'contradiction': [0.5523869395256042], 'neutral': [0.6981598734855652]}}\n","Fruit  Txt: NeutralProb: 0.8978890776634216\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0017631648806855083], 'contradiction': [0.9163099527359009], 'neutral': [0.42571642994880676]}, 'neg': {'entailment': [0.0009953354019671679], 'contradiction': [0.9603596329689026], 'neutral': [0.24306218326091766]}}\n","Seaweed  Txt: NegativeProb: 0.9163099527359009\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.003885776735842228], 'contradiction': [0.7938693761825562], 'neutral': [0.4729103744029999]}, 'neg': {'entailment': [0.004229095298796892], 'contradiction': [0.9151403903961182], 'neutral': [0.9186015725135803]}}\n","Bean  Txt: NeutralProb: 0.4729103744029999\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.019250433892011642], 'contradiction': [0.5095118284225464], 'neutral': [0.8666597008705139]}, 'neg': {'entailment': [0.0042024333961308], 'contradiction': [0.7212520241737366], 'neutral': [0.8453335762023926]}}\n","Legume  Txt: NeutralProb: 0.8666597008705139\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.007391863968223333], 'contradiction': [0.8258443474769592], 'neutral': [0.6338033676147461]}, 'neg': {'entailment': [0.002367192879319191], 'contradiction': [0.9526973962783813], 'neutral': [0.4247056841850281]}}\n","Nut  Txt: NegativeProb: 0.8258443474769592\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0009146817610599101], 'contradiction': [0.9737759828567505], 'neutral': [0.2305496782064438]}, 'neg': {'entailment': [0.001026042620651424], 'contradiction': [0.9799720644950867], 'neutral': [0.23331546783447266]}}\n","Fish  Txt: NegativeProb: 0.9737759828567505\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.002057781908661127], 'contradiction': [0.8461614847183228], 'neutral': [0.470656156539917]}, 'neg': {'entailment': [0.0031428588554263115], 'contradiction': [0.9436665773391724], 'neutral': [0.6832473874092102]}}\n","Beans  Txt: NegativeProb: 0.8461614847183228\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.03194074332714081], 'contradiction': [0.5109742283821106], 'neutral': [0.8697493672370911]}, 'neg': {'entailment': [0.00817123707383871], 'contradiction': [0.7239063382148743], 'neutral': [0.5879546999931335]}}\n","Fruits  Txt: NeutralProb: 0.8697493672370911\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.011984980665147305], 'contradiction': [0.5538082718849182], 'neutral': [0.8203752040863037]}, 'neg': {'entailment': [0.005100894253700972], 'contradiction': [0.7002696990966797], 'neutral': [0.7970885634422302]}}\n","Soy  Txt: NeutralProb: 0.8203752040863037\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.003612876869738102], 'contradiction': [0.9682165384292603], 'neutral': [0.3003537356853485]}, 'neg': {'entailment': [0.0013096337206661701], 'contradiction': [0.975667417049408], 'neutral': [0.16850629448890686]}}\n","Oil  Txt: NegativeProb: 0.9682165384292603\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.012152259238064289], 'contradiction': [0.7176913619041443], 'neutral': [0.8797575831413269]}, 'neg': {'entailment': [0.005399527959525585], 'contradiction': [0.8200892210006714], 'neutral': [0.46687012910842896]}}\n","Coffee  Txt: PositiveProb: 0.8200892210006714\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.021099645644426346], 'contradiction': [0.5747897624969482], 'neutral': [0.8659440875053406]}, 'neg': {'entailment': [0.006253256928175688], 'contradiction': [0.3723839223384857], 'neutral': [0.9342774152755737]}}\n","Spread  Txt: NeutralProb: 0.8659440875053406\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.013855921104550362], 'contradiction': [0.5204052329063416], 'neutral': [0.8396278619766235]}, 'neg': {'entailment': [0.004062116611748934], 'contradiction': [0.7344132661819458], 'neutral': [0.5975995063781738]}}\n","Animal fats  Txt: NeutralProb: 0.8396278619766235\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.009568769484758377], 'contradiction': [0.6199941039085388], 'neutral': [0.7139586210250854]}, 'neg': {'entailment': [0.006060225889086723], 'contradiction': [0.8223893046379089], 'neutral': [0.6042664647102356]}}\n","Oils  Txt: PositiveProb: 0.8223893046379089\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.00869386363774538], 'contradiction': [0.7399929165840149], 'neutral': [0.6606894731521606]}, 'neg': {'entailment': [0.0012172862188890576], 'contradiction': [0.9215177893638611], 'neutral': [0.3136993646621704]}}\n","Vegetables  Txt: PositiveProb: 0.9215177893638611\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.003130478085950017], 'contradiction': [0.8913737535476685], 'neutral': [0.4943259358406067]}, 'neg': {'entailment': [0.0011205169139429927], 'contradiction': [0.9564647674560547], 'neutral': [0.1971675455570221]}}\n","Nuts  Txt: NegativeProb: 0.8913737535476685\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.010088745504617691], 'contradiction': [0.7559530138969421], 'neutral': [0.6154060363769531]}, 'neg': {'entailment': [0.007682600058615208], 'contradiction': [0.927380383014679], 'neutral': [0.5833332538604736]}}\n","Meats  Txt: PositiveProb: 0.927380383014679\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.037641074508428574], 'contradiction': [0.851067841053009], 'neutral': [0.5440548658370972]}, 'neg': {'entailment': [0.0159275121986866], 'contradiction': [0.9243548512458801], 'neutral': [0.28266042470932007]}}\n","Alcoholic beverages  Txt: NegativeProb: 0.851067841053009\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.07262107729911804], 'contradiction': [0.4428078532218933], 'neutral': [0.9440189599990845]}, 'neg': {'entailment': [0.036858320236206055], 'contradiction': [0.776395857334137], 'neutral': [0.8860892653465271]}}\n","Sugar  Txt: NeutralProb: 0.9440189599990845\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.018774403259158134], 'contradiction': [0.6153807044029236], 'neutral': [0.8378666639328003]}, 'neg': {'entailment': [0.006233149208128452], 'contradiction': [0.7982074618339539], 'neutral': [0.7715713381767273]}}\n","Proteins  Txt: NeutralProb: 0.8378666639328003\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.004657689947634935], 'contradiction': [0.7407037019729614], 'neutral': [0.6331037282943726]}, 'neg': {'entailment': [0.0017484241398051381], 'contradiction': [0.9472336173057556], 'neutral': [0.33836960792541504]}}\n","Protein  Txt: PositiveProb: 0.9472336173057556\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.003531295107677579], 'contradiction': [0.7566785216331482], 'neutral': [0.7517404556274414]}, 'neg': {'entailment': [0.004597519990056753], 'contradiction': [0.9095673561096191], 'neutral': [0.863739550113678]}}\n","Roe  Txt: PositiveProb: 0.9095673561096191\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.015617107041180134], 'contradiction': [0.6602728366851807], 'neutral': [0.8620351552963257]}, 'neg': {'entailment': [0.008475931361317635], 'contradiction': [0.8513221144676208], 'neutral': [0.8877391815185547]}}\n","Ovis  Txt: NeutralProb: 0.8620351552963257\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.013805979862809181], 'contradiction': [0.7375701665878296], 'neutral': [0.660952627658844]}, 'neg': {'entailment': [0.003499937243759632], 'contradiction': [0.8866823315620422], 'neutral': [0.38268715143203735]}}\n","Citrus  Txt: PositiveProb: 0.8866823315620422\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0014535720692947507], 'contradiction': [0.9101439118385315], 'neutral': [0.4508104622364044]}, 'neg': {'entailment': [0.0010085820686072111], 'contradiction': [0.9721455574035645], 'neutral': [0.2814461886882782]}}\n","Mushroom  Txt: NegativeProb: 0.9101439118385315\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0038922964595258236], 'contradiction': [0.6717584729194641], 'neutral': [0.643913209438324]}, 'neg': {'entailment': [0.002779936883598566], 'contradiction': [0.7929648756980896], 'neutral': [0.5584001541137695]}}\n","Tea  Txt: NeutralProb: 0.643913209438324\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0019058381440117955], 'contradiction': [0.7672722935676575], 'neutral': [0.6672927737236023]}, 'neg': {'entailment': [0.0012482056627050042], 'contradiction': [0.9107086062431335], 'neutral': [0.29446014761924744]}}\n","Mushrooms  Txt: PositiveProb: 0.9107086062431335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.007782950531691313], 'contradiction': [0.8091585040092468], 'neutral': [0.7436876893043518]}, 'neg': {'entailment': [0.0022058782633394003], 'contradiction': [0.9284095168113708], 'neutral': [0.2871569097042084]}}\n","Fats and oils  Txt: NegativeProb: 0.8091585040092468\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.032096996903419495], 'contradiction': [0.5337309241294861], 'neutral': [0.8590999245643616]}, 'neg': {'entailment': [0.036942530423402786], 'contradiction': [0.5304931998252869], 'neutral': [0.9262622594833374]}}\n","Fat  Txt: NeutralProb: 0.8590999245643616\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0869516059756279], 'contradiction': [0.4777578115463257], 'neutral': [0.9357423186302185]}, 'neg': {'entailment': [0.020304854959249496], 'contradiction': [0.7952901721000671], 'neutral': [0.8235703706741333]}}\n","Beverages  Txt: NeutralProb: 0.9357423186302185\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.0036994945257902145], 'contradiction': [0.8517248034477234], 'neutral': [0.4526064693927765]}, 'neg': {'entailment': [0.0018231479916721582], 'contradiction': [0.9420689940452576], 'neutral': [0.36969491839408875]}}\n","Meat  Txt: NegativeProb: 0.8517248034477234\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.02145012468099594], 'contradiction': [0.5456798076629639], 'neutral': [0.8101754784584045]}, 'neg': {'entailment': [0.010167310945689678], 'contradiction': [0.862823486328125], 'neutral': [0.5442849397659302]}}\n","Legumes  Txt: PositiveProb: 0.862823486328125\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]},{"output_type":"stream","name":"stdout","text":["Diet recommendation has 1 lines\n","{'pos': {'entailment': [0.013816270045936108], 'contradiction': [0.5759490132331848], 'neutral': [0.8443469405174255]}, 'neg': {'entailment': [0.005560094490647316], 'contradiction': [0.6099353432655334], 'neutral': [0.7714299559593201]}}\n","Animal fat  Txt: NeutralProb: 0.8443469405174255\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:allennlp.common.model_card:lerc is not a registered model.\n"]}]},{"cell_type":"code","source":["print(output_dict)\n","\n","for disease in output_dict:\n","  for food in neutral_list:\n","    # output_dict[disease][\"food\"] = food\n","    output_dict[disease][food] = \"neutral\"\n","\n","#converting dict to json\n","with open(\"diet_output.json\", \"w\") as outfile:\n","    json.dump(output_dict, outfile)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"SFNFQx2uveZP","executionInfo":{"status":"error","timestamp":1670116146645,"user_tz":360,"elapsed":247,"user":{"displayName":"vaibhav karanam","userId":"17342897047850185496"}},"outputId":"8600b41f-34fd-4b07-f9da-8bf1fa6003c7"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f3c756f007e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdisease\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mfood\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneutral_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# output_dict[disease][\"food\"] = food\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'output_dict' is not defined"]}]}]}